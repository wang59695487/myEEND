# training options
sampling_rate: 16000
frame_size: 400
frame_shift: 160
model_type: Transformer
max_epochs: 100
gradclip: 5
batchsize: 64
hidden_size: 512
num_frames: 500
num_speakers: 2
input_transform: logmel23_mn
optimizer: noam
lr: 1.0
context_size: 7
subsampling: 10
gradient_accumulation_steps: 1
transformer_encoder_n_heads: 8
transformer_encoder_n_layers: 8
transformer_encoder_dropout: 0.1
noam_warmup_steps: 200000
seed: 777
gpu: 1
